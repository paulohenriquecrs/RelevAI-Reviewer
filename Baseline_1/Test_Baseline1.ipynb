{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Kit - Relevance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Challenge\n",
    "The RelevAI project, which is in the field of Information Retrieval (IR), focuses on academic literature. It is driven by the need to classify the relevance of papers in the scientific literature, a task that grows harder with large text volumes and diverse topics. The project aims to automate the process of assessing document relevance to specific prompts or queries, which has the potential to save users valuable time and effort.\n",
    "\n",
    "Traditional methods like text matching and keyword search often miss contextual meaning and word relationships, resulting in inaccurate relevance assessments. Therefore, this challenge proposes users to come up with Artificial Intelligence techniques to improve this task, capturing semantic and contextual aspects of the text to better predict its relevance. The successful development of an effective relevance ranking algorithm can have a significant impact on society. It can enhance literature reviews, accelerate scientific research, expedite breakthroughs in diverse fields, facilitate efficient literature searches, simplify information access, and enhance learning and decision-making processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data\n",
    "This challenge aims to categorize candidate papers based on their alignment with a given prompt. It is formulated as a supervised learning problem, where the input consists of a prompt along with 4 candidate survey papers, and the desired output are the relevance classes to be assigned to each of these papers. Specifically, label 3 denotes the most relevance, 2 for the second most relevance, 1 for the second least relevance, and 0 for the least relevance. Each paper contains a Title, Abstract, and Related Works sections generated by ChatGPT from citations. The prompts were generated by reverse-engineering using ChatGPT and the most relevant paper as a reference.\n",
    "\n",
    "### Input\n",
    "Pairs made out of a prompt + paper (title, abstract and related works). Note: each prompt has 4 different papers with different levels of relevance (most relevant, second most relevant, second least relevant and least relevant). Therefore, each prompt appears in four different instances of the dataset, but with different papers in each row (and therefore different labels representing the relevance level).\n",
    "\n",
    "### Output\n",
    "Numeric value indicating the level of relevance of a paper with regard to a prompt:\n",
    "1. Label 3 indicates the highest relevance (most relevant)\n",
    "2. Label 2 for the second highest (second most relevant)\n",
    "3. Label 1 for the second lowest (second least relevant)\n",
    "4. Label 0 for the lowest relevance (least relevant)e.age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Initially, the performance of the models will be evaluated using Kendall’s Tau coefficient, which is further explained down below in this notebook. Therefore, you should prioritize approaches that optimize the Kendall’s Tau coefficient. We also present different metrics in the scoring report, such as accuracy and precision, to give a further idea of how well your model is performing. Those other metrics can be used as tie breakers in case of different approaches with the same Kendall’s Tau coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Starting kit notebook\n",
    "\n",
    "Now that you have an idea about the scope of the challenge, motivations and objectives behind it, data format and evaluation, you may want to get a first taste of the challenge. This starting kit provides a basic baseline using a small sample of the data so that you may better visualize and understand the date, besides being a great example of how you should generate a zip file to submit in the Codabench website. You may play with it in order to get familiar before you start using your creativity to develop your very own solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "This challenge wouldn't be possible without the hard work of different people involved, so we use this section to acknowledge the responsibles for creating this project.\n",
    "\n",
    "### Database donors\n",
    "* Benedictus Kent Rachmat\n",
    "* KHUONG Thanh Gia Hieu\n",
    "\n",
    "### Project development team members\n",
    "* Paulo Henrique Couto\n",
    "* Nageeta Kumari \n",
    "* Quang Phuoc HO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started Instructions\n",
    "\n",
    "Before executing the code, we want you to make sure that you have a required setup to run this starting kit.\n",
    "1. Make sure you are using python 3.9 version  \n",
    "2. All the following packages must be installed, to check these you can simply use: pip show [name_of_package]  \n",
    "    - numpy\n",
    "    - pandas\n",
    "    - matplotlib\n",
    "    - tqdm\n",
    "    - sentence_transformers\n",
    "    - sklearn\n",
    "    - torch\n",
    "    - ipywidgets\n",
    "    - wordcloud\n",
    "\n",
    "If any of these are not installed, simply use: pip install [name_of_package]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Setup\n",
    "***\n",
    "`COLAB` determines whether this notebook is running on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB='google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # clone github repo\n",
    "    !git clone https://github.com/ihsaan-ullah/M1-Challenge-Class-2024.git\n",
    "\n",
    "    # move to the HEP starting kit folder\n",
    "    %cd M1-Challenge-Class-2024/Relevance/Starting_Kit/\n",
    "\n",
    "    !pip install -q --upgrade sentence-transformers transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Imports\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Directories\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./\"\n",
    "# Input data directory to read training data from\n",
    "input_dir = root_dir + \"sample_data/\"\n",
    "# Reference data directory to read test labels from\n",
    "reference_dir = root_dir + \"sample_data/\"\n",
    "# Output data directory to write predictions to\n",
    "output_dir = root_dir + \"sample_result_submission\"\n",
    "# Program directory\n",
    "program_dir = root_dir + \"ingestion_program\"\n",
    "# Score directory\n",
    "score_dir = root_dir + \"scoring_program\"\n",
    "# Directory to read submitted submissions from\n",
    "submission_dir = root_dir + \"sample_code_submission\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Add directories to path\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(input_dir)\n",
    "sys.path.append(reference_dir)\n",
    "sys.path.append(output_dir)\n",
    "sys.path.append(program_dir)\n",
    "sys.path.append(submission_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Data\n",
    "***\n",
    "1. Load Data\n",
    "2. Preprocess data\n",
    "\n",
    "\n",
    "### ⚠️ Note:\n",
    "The data used here is sample data is for demonstration only to get a view of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "      self.df = None\n",
    "\n",
    "      print(\"==========================================\")\n",
    "      print(\"Data\")\n",
    "      print(\"==========================================\")\n",
    "\n",
    "  def load_data(self):\n",
    "    \"\"\"\n",
    "      Loads data from csv file\n",
    "    \"\"\"\n",
    "    print(\"[*] Loading Data\")\n",
    "\n",
    "    # data file path\n",
    "    data_file = os.path.join(input_dir, 'relevance_sample_data.csv')\n",
    "    \n",
    "    # read data\n",
    "    self.df = pd.read_csv(data_file)\n",
    "\n",
    "\n",
    "  def _text_to_dict(self, text):\n",
    "    \"\"\"\n",
    "    Converts a text string into a dictionary.\n",
    "\n",
    "    :param text: A string representation of a dictionary.\n",
    "    :return: A dictionary object if conversion is successful, otherwise {}.\n",
    "    \"\"\"\n",
    "    try:\n",
    "      return ast.literal_eval(text)\n",
    "    except:\n",
    "      return {}  # Return an empty dictionary in case of an error\n",
    "    \n",
    "  def _dict_to_paragraphs(self, dictionary):\n",
    "    \"\"\"\n",
    "    Converts a dictionary into a string of paragraphs.\n",
    "\n",
    "    :param dictionary: A dictionary.\n",
    "    :return: A string composed of paragraphs based on the dictionary's key-value pairs.\n",
    "    \"\"\"\n",
    "    text = ''\n",
    "    for i, (k, v) in enumerate(dictionary.items()):\n",
    "        text += k.capitalize() + '\\n' + v + '\\n'\n",
    "    return text\n",
    "  \n",
    "  def transfrom_data(self):\n",
    "\n",
    "    print(\"[*] Transforming Data\")\n",
    "    \n",
    "    # Convert to dictionary\n",
    "    self.df['most_relevant_dict'] = self.df['most_relevant'].apply(self._text_to_dict)\n",
    "    self.df['second_most_relevant_dict'] = self.df['second_most_relevant'].apply(self._text_to_dict)\n",
    "    self.df['second_least_relevant_dict'] = self.df['second_least_relevant'].apply(self._text_to_dict)\n",
    "    self.df['least_relevant_dict'] = self.df['least_relevant'].apply(self._text_to_dict)\n",
    "\n",
    "\n",
    "    # Convert from dictionary to text\n",
    "    self.df['most_relevant_text'] = self.df['most_relevant_dict'].apply(self._dict_to_paragraphs)\n",
    "    self.df['second_most_relevant_text'] = self.df['second_most_relevant_dict'].apply(self._dict_to_paragraphs)\n",
    "    self.df['second_least_relevant_text'] = self.df['second_least_relevant_dict'].apply(self._dict_to_paragraphs)\n",
    "    self.df['least_relevant_text'] = self.df['least_relevant_dict'].apply(self._dict_to_paragraphs)\n",
    "\n",
    "    \n",
    "  def pad_sequence_to_max_length(self, sequence, padding_value=0):\n",
    "    \"\"\"\n",
    "    this functions pads an input sequence\n",
    "    \"\"\"\n",
    "    padded_sequence = sequence + [padding_value] * (self.max_length - len(sequence))\n",
    "    return padded_sequence\n",
    "\n",
    "\n",
    "  def prepare_data(self):\n",
    "\n",
    "    print(\"[*] Prepare Data for Training\")\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    least_relevant_df = pd.DataFrame()\n",
    "    least_relevant_df['input'] = self.df.progress_apply(lambda row: tokenizer.encode(row['prompt'] + \" \" + row['least_relevant_text'], truncation=True), axis=1)\n",
    "    least_relevant_df['label'] = 0\n",
    "    \n",
    "    second_least_relevant_df = pd.DataFrame()\n",
    "    second_least_relevant_df['input'] = self.df.progress_apply(lambda row: tokenizer.encode(row['prompt'] + \" \" + row['second_least_relevant_text'], truncation=True), axis=1)\n",
    "    second_least_relevant_df['label'] = 1\n",
    "    \n",
    "    second_most_relevant_df = pd.DataFrame()\n",
    "    second_most_relevant_df['input'] = self.df.progress_apply(lambda row: tokenizer.encode(row['prompt'] + \" \" + row['second_most_relevant_text'], truncation=True), axis=1)\n",
    "    second_most_relevant_df['label'] = 2\n",
    "    \n",
    "    most_relevant_df = pd.DataFrame()\n",
    "    most_relevant_df['input'] = self.df.progress_apply(lambda row: tokenizer.encode(row['prompt'] + \" \" + row['most_relevant_text'], truncation=True), axis=1)\n",
    "    most_relevant_df['label'] = 3\n",
    "\n",
    "    # combine all 4 dataframes\n",
    "    tokenized_df = pd.concat([least_relevant_df, second_least_relevant_df, second_most_relevant_df, most_relevant_df], ignore_index=True)\n",
    "    \n",
    "    tokenized_df['label_onehot'] = tokenized_df['label'].apply(lambda x: list(np.eye(4)[x]))\n",
    "\n",
    "    # shuffle combined dataframe\n",
    "    tokenized_df = tokenized_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # pad the sequences\n",
    "    self.max_length=tokenized_df['input'].apply(len).max()\n",
    "    tokenized_df['input_padded'] = tokenized_df['input'].apply(lambda x: self.pad_sequence_to_max_length(x))\n",
    "    \n",
    "    X = torch.tensor(tokenized_df['input_padded'].tolist())\n",
    "    y = torch.tensor(tokenized_df['label_onehot'].tolist())\n",
    "\n",
    "    dataset = TensorDataset(X, y)\n",
    "    print(tokenized_df)\n",
    "    self.train_dataset, self.test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "  def get_train_data(self):\n",
    "    return self.train_dataset\n",
    "  \n",
    "  def get_test_data(self):\n",
    "    return self.test_dataset\n",
    "  \n",
    "  def show_random_sample(self):\n",
    "    random_sample_index = np.random.randint(0, len(self.df))\n",
    "\n",
    "    print(\"Prompt:\\n\", self.df.iloc[random_sample_index]['prompt'], \"...\\n\")\n",
    "    print(\"Most Relevant Text:\\n\", self.df.iloc[random_sample_index]['most_relevant_text'][:300], \"...\\n\")\n",
    "    print(\"Second Most Relevant Text:\\n\", self.df.iloc[random_sample_index]['second_most_relevant_text'][:300], \"...\\n\")\n",
    "    print(\"Second Least Relevant Text:\\n\", self.df.iloc[random_sample_index]['second_least_relevant_text'][:300], \"...\\n\")\n",
    "    print(\"Least Relevant Text:\\n\", self.df.iloc[random_sample_index]['least_relevant_text'][:300], \"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Data\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "# Initilaize data\n",
    "data = Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loading Data\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Transforming Data\n"
     ]
    }
   ],
   "source": [
    "# transform data\n",
    "data.transfrom_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Prepare Data for Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b945b50bfcc48d398e420b6a5a897a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54162e36c8641b78d6479f0ef4251f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f83ff1d56e4683abdc03ed5bb6ba97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913c2515f6fa4fe7becbbcf3aa3b1027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  input  label  \\\n",
      "0     [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...      3   \n",
      "1     [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...      0   \n",
      "2     [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...      2   \n",
      "3     [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...      1   \n",
      "4     [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...      2   \n",
      "...                                                 ...    ...   \n",
      "1995  [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...      2   \n",
      "1996  [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...      2   \n",
      "1997  [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...      1   \n",
      "1998  [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...      2   \n",
      "1999  [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...      2   \n",
      "\n",
      "              label_onehot                                       input_padded  \n",
      "0     [0.0, 0.0, 0.0, 1.0]  [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...  \n",
      "1     [1.0, 0.0, 0.0, 0.0]  [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...  \n",
      "2     [0.0, 0.0, 1.0, 0.0]  [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...  \n",
      "3     [0.0, 1.0, 0.0, 0.0]  [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...  \n",
      "4     [0.0, 0.0, 1.0, 0.0]  [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...  \n",
      "...                    ...                                                ...  \n",
      "1995  [0.0, 0.0, 1.0, 0.0]  [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...  \n",
      "1996  [0.0, 0.0, 1.0, 0.0]  [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...  \n",
      "1997  [0.0, 1.0, 0.0, 0.0]  [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...  \n",
      "1998  [0.0, 0.0, 1.0, 0.0]  [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...  \n",
      "1999  [0.0, 0.0, 1.0, 0.0]  [101, 4339, 1037, 11778, 5002, 2030, 19184, 20...  \n",
      "\n",
      "[2000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "data.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      " Write a systematic survey or overview about the impact of electrical stimulation on three‐dimensional myoblast cultures, focusing on the differentiation potential and gene expression of key markers such as MyoD, myogenin, and AChR as measured by real‐time RT‐PCR. Consider the implications for tissue engineering and the potential for clinical applications in treating focal skeletal muscle diseases. ...\n",
      "\n",
      "Most Relevant Text:\n",
      " Title\n",
      "Impact of electrical stimulation on three‐dimensional myoblast cultures ‐ a real‐time RT‐PCR study\n",
      "Abstract\n",
      "Several focal skeletal muscle diseases, including tumours and trauma lead to a limited loss of functional muscle tissue. There is still no suitable clinical approach for treating such de ...\n",
      "\n",
      "Second Most Relevant Text:\n",
      " Title\n",
      "Engineering skeletal myoblasts: roles of three-dimensional culture and electrical stimulation.\n",
      "Abstract\n",
      "Immature skeletal muscle cells, or myoblasts, have been used in cellular cardiomyoplasty in attempts to regenerate cardiac muscle tissue by injection of cells into damaged myocardium. In som ...\n",
      "\n",
      "Second Least Relevant Text:\n",
      " Title\n",
      "Wildlife conservation and reproductive cloning.\n",
      "Abstract\n",
      "Reproductive cloning, or the production of offspring by nuclear transfer, is often regarded as having potential for conserving endangered species of wildlife. Currently, however, low success rates for reproductive cloning limit the pract ...\n",
      "\n",
      "Least Relevant Text:\n",
      " Title\n",
      "Use of Infrared Photography to Visualize a Tattoo for Identification in Advanced Decomposition\n",
      "Abstract\n",
      "AUTHORS Matthew D. Cain MD, University of Alabama at Birmingham Pathology Roles: Project conception and/or design, data acquisition, analysis and/or interpretation, manuscript creation and/o ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show random sample from data\n",
    "data.show_random_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train test data\n",
    "train_dataset = data.get_train_data()\n",
    "test_dataset = data.get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Import Submission Model\n",
    "***\n",
    "We import a class named `Model` from the submission file (`model.py`). This `Model` class has the following methods:\n",
    "- `init`: initializes classifier\n",
    "- `fit`: gets train data and labels as input to train the classifier\n",
    "- `predict`: gets test data and outputs predictions made by the trained classifier\n",
    "\n",
    "\n",
    "In this example code, the `Model` class implements a Gradient Boosting Classifier model. You can find the code in `M1-Challenge-Class-2024/Relevance/Starting_Kit/sample_code_submission/model.py`. You can modify it the way you want, keeping the required class structure and functions there. More instructions are given inside the `model.py` file. If running in Collab, click the folder icon in the left sidebar to open the file browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Program\n",
    "***\n",
    "**`Ingestion program`** is responsible to run the submission of a participant on Codabench platform. **`Program`** is a simplified version of the **Ingestion Program** to show to participants how it runs a submission.\n",
    "1. Train a model on train data\n",
    "2. Predict using Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Program():\n",
    "\n",
    "    def __init__(self, data):\n",
    "\n",
    "        # used to keep object of Model class to run the submission\n",
    "        self.model = None\n",
    "        # object of Data class used here to get the train and test sets\n",
    "        self.data = data\n",
    "\n",
    "        # results\n",
    "        self.results = []\n",
    "\n",
    "        print(\"==========================================\")\n",
    "        print(\"Program\")\n",
    "        print(\"==========================================\")\n",
    "    \n",
    "    def initialize_submission(self):\n",
    "        print(\"[*] Initializing Submmited Model\")\n",
    "        self.model = Model()\n",
    "        ##############################################################################################\n",
    "        # This part will go in model.py so that it will be called by self.model = Model()\n",
    "        ##############################################################################################\n",
    "        \n",
    "\n",
    "    def fit_submission(self):\n",
    "        print(\"[*] Calling fit method of submitted model\")\n",
    "        train_dataloader = DataLoader(self.data.get_train_data(), batch_size=8, shuffle=True)\n",
    "        num_epochs=1\n",
    "        self.model.fit(train_dataloader, num_epochs)\n",
    "        ##############################################################################################\n",
    "        # This part will go in model.py so that it will be called by self.model.fit(train_dataloader)\n",
    "        ##############################################################################################\n",
    "\n",
    "\n",
    "    def predict_submission(self):\n",
    "        print(\"[*] Calling predict method of submitted model\")\n",
    "        test_dataloader = DataLoader(self.data.get_test_data(), batch_size=8, shuffle=False)\n",
    "        self.y_test_hat, self.y_test_truth = self.model.predict(test_dataloader)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Program\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "# Intiialize Program\n",
    "program = Program(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Initializing Submmited Model\n",
      "[*] - Initializing Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize submitted model\n",
    "program.initialize_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Calling fit method of submitted model\n",
      "[*] - Training Classifier on the train set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1766f37047af48dda85bcad7d5adda12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Call fit method of submitted model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mprogram\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 29\u001b[0m, in \u001b[0;36mProgram.fit_submission\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mget_train_data(), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\M1-AI-Challenge---Relevance-Group\\Relevance\\Starting_Kit\\./sample_code_submission\\model.py:83\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, dataloader, num_epochs)\u001b[0m\n\u001b[0;32m     80\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     81\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 83\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     86\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\M1_AI-Challenge\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\M1_AI-Challenge\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Call fit method of submitted model\n",
    "program.fit_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call predict method of submitted model\n",
    "program.predict_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Score\n",
    "***\n",
    "\n",
    "The model’s performance is evaluated using several metrics, including precision, recall, F1-score, and Kendall’s Tau. These metrics provide a comprehensive view of the model’s performance across all classes. Precision measures the model’s exactness, recall measures its completeness, the F1-score balances precision and recall, and Kendall’s Tau measures the correlation between the predicted and actual rankings. \n",
    "\n",
    "1.\tPrecision: Precision is the ratio of correctly predicted positive observations to the total predicted positives. It is also known as Positive Predictive Value. It is a measure of a classifier’s exactness. Low precision indicates a high number of false positives.\n",
    "\n",
    "2.\tRecall (Sensitivity): Recall is the ratio of correctly predicted positive observations to all the actual positives. It is also known as Sensitivity, Hit Rate, or True Positive Rate. It is a measure of a classifier’s completeness. Low recall indicates a high number of false negatives.\n",
    "\n",
    "3.\tF1 Score: The F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. It is suitable for uneven class distribution problems.\n",
    "\n",
    "4.\tSupport: Support is the number of actual occurrences of the class in the specified dataset. Imbalanced support in the training data may indicate structural weaknesses in the reported scores of the classifier and could indicate the need for stratified sampling or rebalancing.\n",
    "\n",
    "5.\tAccuracy: Accuracy is the most intuitive performance measure. It is simply a ratio of correctly predicted observation to the total observations. One may think that if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost the same.\n",
    "\n",
    "6.\tMacro Avg: Macro-average method can be used when you want to know how the model performs overall across the sets of data. You should not come up with any specific decision with this average.\n",
    "\n",
    "7.\tWeighted Avg: In contrary to macro-average method, if you want to know how the model is performing with respect to the most frequent class, you should use a weighted-average method. This will aggregate the contributions of all classes to compute the average metric.\n",
    "\n",
    "8.\tKendall’s Tau: Kendall’s Tau is a measure of correlation. It gives a value between -1 and 1. A value close to 1 means that there’s a strong positive correlation between the two variables, and a value close to -1 means that there’s a strong negative correlation. A value close to 0 means that there’s no correlation. In your case, the value is approximately 0.70, indicating a good agreement.\n",
    "\n",
    "More info in: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Score():\n",
    "\n",
    "    def __init__(self, data, program):\n",
    "\n",
    "        self.data = data\n",
    "        self.program = program\n",
    "\n",
    "        print(\"==========================================\")\n",
    "        print(\"Score\")\n",
    "        print(\"==========================================\")\n",
    "\n",
    "    def compute_scores(self):\n",
    "        print(\"[*] Computing scores\")\n",
    "\n",
    "        y_test_ = np.array(self.program.y_test_truth)\n",
    "        y_test = np.argmax(y_test_, axis=1)\n",
    "        y_test_hat = self.program.y_test_hat\n",
    "\n",
    "        # Classification report\n",
    "        print(classification_report(y_test, y_test_hat))\n",
    "\n",
    "        k_tau, _ = kendalltau(y_test, y_test_hat)\n",
    "        print(f\"Kendall's Tau: {round(k_tau, 3)}\")\n",
    "\n",
    "        kappa = cohen_kappa_score(y_test, y_test_hat)\n",
    "        print(f\"Cohen's Kappa: {round(kappa, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Score\n",
    "score = Score(data=data, program=program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Score\n",
    "score.compute_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Submissions\n",
    "***\n",
    "\n",
    "### **Unit Testing**\n",
    "\n",
    "It is <b><span style=\"color:red\">important that you test your submission files before submitting them</span></b>. All you have to do to make a submission is modify the file <code>model.py</code> in the <code>sample_code_submission/</code> directory, then run this test to make sure everything works fine. This is the actual program that will be run on the server to test your submission.\n",
    "<br>\n",
    "Keep the sample code simple.<br>\n",
    "\n",
    "<code>python3</code> is required for this step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test Ingestion Program**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################\n",
      "### Ingestion Program\n",
      "############################################\n",
      "\n",
      "[*] Loading Data\n",
      "[*] Transforming Data\n",
      "[*] Prepare Data for Training\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n",
      "[*] Initializing Submmited Model\n",
      "[*] - Initializing Classifier\n",
      "[*] Calling fit method of submitted model\n",
      "[*] - Training Classifier on the train set\n",
      "Epoch 1:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "Average training loss: 0.4224155795984552\n",
      "[*] Calling predict method of submitted model\n",
      "[*] - Predicting test set using trained Classifier\n",
      "[*] Saving ingestion result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Paulo Couto\\M1-AI-Challenge---Relevance-Group\\Relevance\\Starting_Kit\\ingestion_program\\ingestion.py\", line 262, in <module>\n",
      "    ingestion.save_result()\n",
      "  File \"C:\\Users\\Paulo Couto\\M1-AI-Challenge---Relevance-Group\\Relevance\\Starting_Kit\\ingestion_program\\ingestion.py\", line 223, in save_result\n",
      "    \"predictions\": self.y_test_hat.tolist(),\n",
      "AttributeError: 'list' object has no attribute 'tolist'\n"
     ]
    }
   ],
   "source": [
    "!python $program_dir/ingestion.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test Scoring Program**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################\n",
      "### Scoring Program\n",
      "############################################\n",
      "\n",
      "[*] Reading predictions\n",
      "[OK]\n",
      "[*] Computing scores\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.57      0.54        96\n",
      "           1       0.61      0.53      0.57       107\n",
      "           2       0.57      0.62      0.59        91\n",
      "           3       0.77      0.74      0.75       106\n",
      "\n",
      "    accuracy                           0.61       400\n",
      "   macro avg       0.62      0.61      0.61       400\n",
      "weighted avg       0.62      0.61      0.62       400\n",
      "\n",
      "Kendall's Tau: 0.6974695674996462\n",
      "[OK]\n",
      "[*] Writing scores\n",
      "[OK]\n",
      "\n",
      "---------------------------------\n",
      "[OK] Total duration: 0:00:00.026932\n",
      "---------------------------------\n",
      "\n",
      "----------------------------------------------\n",
      "[OK] Scoring Program executed successfully!\n",
      "----------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python $score_dir/score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prepare the submission**\n",
    "\n",
    "TODOs:  \n",
    "- The following submission will be submitted by the participants to your competition website. Describe this clearly and point to the competition once your website is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit : Relevance-Baseline1-Sample_Data-Code_Submission_24-01-31-00-17.zip to the competition\n",
      "You can find the zip file in `M1-Challenge-Class-2024/Relevance/Starting_Kit/\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from data_io import zipdir\n",
    "the_date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "code_submission = 'Relevance-Baseline1-Sample_Data-Code_Submission_' + the_date + '.zip'\n",
    "zipdir(code_submission, submission_dir)\n",
    "print(\"Submit : \" + code_submission + \" to the competition\")\n",
    "print(\"You can find the zip file in `M1-Challenge-Class-2024/Relevance/Starting_Kit/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9e001b0608738f9411416229c98988c04b997dc526fb61c5e4e084e768e3249"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
